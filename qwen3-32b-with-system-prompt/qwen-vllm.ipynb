{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9d60c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%% capture\n",
    "!pip install pandas vllm transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334e6d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33226ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_boxed(text):\n",
    "    m = re.search(r'\\\\boxed\\{([^}]*)\\}', text)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def extract_after_think(text: str):\n",
    "    m = re.search(r'</think>\\s*([\\s\\S]*)', text)\n",
    "    return m.group(1).strip() if m else None\n",
    "\n",
    "def extract_answer_xml(text: str):\n",
    "    m = re.search(r'<final_answer>([\\s\\S]*?)</final_answer>', text)\n",
    "    return m.group(1).strip() if m else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f99258",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdcc3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ac6642",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"Qwen/Qwen3-32B-AWQ\"\n",
    "\n",
    "max_model_len = 16384\n",
    "max_token = 16384\n",
    "gpu_memory_utilization = 0.9\n",
    "gpu_count = 1\n",
    "\n",
    "enable_thinking = True\n",
    "\n",
    "# Recommended for Qwen3 with thinking enabled\n",
    "temperature = 0.6\n",
    "top_p = 0.95\n",
    "top_k = 20\n",
    "min_p = 0\n",
    "\n",
    "# # Recommended for Qwen3 with thinking disabled\n",
    "# temperature = 0.7\n",
    "# top_p = 0.8\n",
    "# top_k = 20\n",
    "# min_p = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f84f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = r\"\"\"You are a highly specialized AI assistant with expert-level proficiency in financial, compliance, investment, and regulatory examination topics, capable of operating fluently in both Thai and English. Your primary function is to analyze complex queries and deliver responses with the highest degree of accuracy, precision, and relevance.\n",
    "\n",
    "### Core Objectives\n",
    "\n",
    "- You must reason logically, utilize your domain expertise, and **ensure that all answers are factually correct**.\n",
    "\n",
    "- You must **always provide an answer**. A correct answer **always exists**. If uncertain, use logic and context to infer the most likely answer.\n",
    "\n",
    "- For multiple-choice questions, **output only the letter identifier** (A, B, C, or D). **Do not include the option text.**\n",
    "\n",
    "- For binary classification questions (e.g., stock prediction), output only the **exact answer string provided in the prompt** (e.g., `Rise`, `Fall`, `ขึ้น`, `ลง`, `เพิ่มขึ้น`, `ลดลง`).\n",
    "\n",
    "- **Present your final answer enclosed within <final_answer></final_answer> XML tags.**\n",
    "\n",
    "### Question type\n",
    "\n",
    "There are two main types of questions:\n",
    "\n",
    "1. Multiple-choice: **A, B, C, D**. You must output only the letter identifier.\n",
    "2. Rise or Fall classification: Given the scenario, you must predict whether the closing price of a specific stock (e.g., $INTC) will increase (“Rise”) or decrease (“Fall”). Depending on the question, the choices will be either:\n",
    "    - \"Rise\" or \"Fall\"\n",
    "    - \"ขึ้น\" or \"ลง\"\n",
    "    - \"เพิ่มขึ้น\" or \"ลดลง\"\n",
    "    \n",
    "    You must answer only use **exact given word** as your final answer.\n",
    "\n",
    "### Example\n",
    "\n",
    "**Example Thai Question 1 (Multiple-choice)**\n",
    "\n",
    "Question:\n",
    "```\n",
    "ตอบคำถามด้วยตัวเลือกที่เหมาะสม A, B, C และ D โปรดตอบด้วยคำตอบที่ถูกต้องแม่นยำคือ A, B, C หรือ D เท่านั้น อย่าอธิบายเยิ่นเย้อหรือให้ข้อมูลเพิ่มเติม\n",
    "คำถาม: ค่าครองชีพในประเทศที่ใช้ดอลลาร์แพงกว่าประเทศไทยหรือไม่\n",
    "ตัวเลือกคำตอบ: A: ใช่, B: ไม่, C: ไม่อาจทราบได้, D: ถูกทุกข้อ\n",
    "คำตอบ:\n",
    "```\n",
    "Expected Answer:\n",
    "```\n",
    "<final_answer>A</final_answer>\n",
    "```\n",
    "---\n",
    "\n",
    "**Example Thai Question 2 (Rise or Fall classification)**\n",
    "\n",
    "Question:\n",
    "```\n",
    "วิเคราะห์ข้อมูลและทวีตเพื่อสรุปว่าราคาปิดของ $gs จะปรับตัวขึ้นหรือลงในวันที่ 2017-12-20 โปรดยืนยันว่าขึ้นหรือลง\n",
    "\n",
    "บริบท: วันที่, เปิด, ปิด\n",
    "Jan 2 2023, 100, 102\n",
    "Jan 3 2023, 102, 105\n",
    "Jan 4 2023, 105, 103\n",
    "Jan 5 2023, 103, 106\n",
    "\n",
    "2017-12-06: Analysts say $ABC could hit $110 soon. Strong buy recommendation.\n",
    "```\n",
    "Expected Answer:\n",
    "```\n",
    "<final_answer>ขึ้น</final_answer>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Example English Question 1 (Multiple-choice)**\n",
    "\n",
    "Question:\n",
    "```\n",
    "Answer the question with the appropriate options A, B, C and D. Please respond with the exact answer A, B, C or D only. Do not be verbose or provide extra information. \n",
    "Question: Which currency is more valuable, the US dollar or the Thai baht?\n",
    "Answer Choices: A: Thai baht, B: US dollar, C: Both have same value, D: All of that above\n",
    "Answer:\n",
    "```\n",
    "Expected Answer:\n",
    "```\n",
    "<final_answer>B</final_answer>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Example English Question 2 (Rise or Fall classification)**\n",
    "\n",
    "Question:\n",
    "```\n",
    "Given the data and tweets, could you project whether the closing price of $intc will grow or shrink at 2018-01-23? Please specify either Rise or Fall.\n",
    "\n",
    "Context: Date, Open, Close\n",
    "Jan 2 2023, 110, 102\n",
    "Jan 3 2023, 112, 105\n",
    "Jan 4 2023, 115, 103\n",
    "Jan 5 2023, 113, 106\n",
    "\n",
    "2017-12-06: Analysts say $ABC could hit $100 soon.\n",
    "```\n",
    "Expected Answer:\n",
    "```\n",
    "<final_answer>Fall</final_answer>\n",
    "```\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3ce643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "if max_token > max_model_len:\n",
    "    print(f\"Warning: max_token {max_token} is greater than max_model_len {max_model_len}. Setting max_token to max_model_len.\")\n",
    "    max_token = max_model_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b113792e",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01400ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dtype = \"bfloat16\" if torch.cuda.is_bf16_supported() else \"float16\"\n",
    "except:\n",
    "    dtype = \"float16\"\n",
    "    \n",
    "print(f\"Dtype: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c4b5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(INPUT_FILE)  \n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641474c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init LLM\n",
    "prompts = df[\"query\"].tolist()\n",
    "\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    top_k=top_k,\n",
    "    min_p=min_p,\n",
    "    max_tokens=(max_token)\n",
    ")\n",
    "\n",
    "llm = LLM(\n",
    "    model=model,    \n",
    "    dtype=dtype,             \n",
    "    gpu_memory_utilization=gpu_memory_utilization,\n",
    "    max_model_len=max_model_len,\n",
    "    tensor_parallel_size=gpu_count\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328a70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply conversations template\n",
    "\n",
    "system_msg = {\"role\": \"system\", \"content\": SYSTEM_PROMPT}\n",
    "\n",
    "conversations = [\n",
    "    [\n",
    "      system_msg,\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    for prompt in prompts\n",
    "]\n",
    "\n",
    "input_texts = [tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=enable_thinking,  # Set to False to strictly disable thinking\n",
    ") for messages in conversations]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffc68f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49d2559",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a02905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate responses\n",
    "\n",
    "print(f\"Generating {len(prompts)} completions\")\n",
    "outputs = llm.generate(input_texts, sampling_params=sampling_params)\n",
    "\n",
    "responses = [out.outputs[0].text for out in outputs]\n",
    "\n",
    "# === 5a. Attach back to original DataFrame ===\n",
    "# Make sure the order is the same\n",
    "df[\"response\"] = responses\n",
    "df[\"answer\"] = df[\"response\"].apply(extract_answer_xml)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db3dafb",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c278183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "OUTPUT_FILE = \"output.csv\"\n",
    "SUBMIT_FILE = \"submit.csv\"\n",
    "\n",
    "df.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8\")\n",
    "print(f\"Saved results to {OUTPUT_FILE}\")\n",
    "\n",
    "df[[\"id\", \"answer\"]].to_csv(SUBMIT_FILE, index=False, encoding=\"utf-8\")\n",
    "print(f\"Saved submit file to {SUBMIT_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
